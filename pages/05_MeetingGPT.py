import streamlit as st
import subprocess
from pydub import AudioSegment
import math
import glob
import os
import openai
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import StrOutputParser
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores.faiss import FAISS
from langchain.storage import LocalFileStore
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda


llm = ChatOpenAI(
    temperature=0.1,
)

has_transcript = os.path.exists("./.cache/eng_vedio.txt")

splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
                chunk_size = 1000,
                chunk_overlap = 200,
            )



@st.cache_data()
def extract_audio_from_video(video_path, audio_path):
    if has_transcript:
        return
    
    command = ["/opt/homebrew/bin/ffmpeg", "-i", video_path, "-vn", audio_path, "-y"]
    subprocess.run(command)

@st.cache_data()
def cut_audio_in_chunks(audio_path, chunk_size, chunks_folder):
    if has_transcript:
        return
    
    track = AudioSegment.from_mp3(audio_path)
    chunk_len = chunk_size * 60 * 1000
    chunks = math.ceil(len(track)/chunk_len)

    for i in range(chunks):
        start_time = i * chunk_len
        end_time = (i + 1) * chunk_len
        chunk = track[start_time:end_time]
        chunk.export(f"{chunks_folder}/{i}_chunk.mp3", format="mp3")

@st.cache_data()
def transcribe_chunks(chunk_folder, destination):
    if has_transcript:
        return
    
    files = glob.glob(f"{chunk_folder}/*.mp3")
    files.sort()
    for file in files:
        with open(file, "rb") as audio_file, open(destination, "a") as text_file:
            transcript = openai.Audio.transcribe(
                "whisper-1",
                audio_file,
            )
            text_file.write(f'{transcript["text"]} \n')

@st.cache_data(show_spinner="Embedding text...")
def embed_file(file_path, file_name):
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file_name}")
    loader = TextLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    embedding = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embedding, cache_dir)
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()
    
    return retriever

def save_message(message, role):
    st.session_state["messages"].append({"message":message, "role": role})

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def paint_history():
    for message in st.session_state["messages"]:
        send_message(message["message"], message["role"], save=False)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

st.set_page_config(
    page_title="MeetingGPT",
    page_icon="ğŸ™ï¸",
)

st.markdown(
    """
# MeetingGPT

## Welcome Meeting GPT !ğŸ™†â€â™€ï¸ \n
ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì„¸ìš” ! \n
ê·¸ëŸ¼ í…ìŠ¤íŠ¸ë¡œ ëœ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë°›ì„ ìˆ˜ ìˆì–´ìš” \n
ë¹„ë””ì˜¤ ë‚´ìš© ìš”ì•½ë³¸ì„ ë°›ê±°ë‚˜, ë¹„ë””ì˜¤ì— ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•˜ë©´ ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤ ğŸ˜

##### ì¢Œì¸¡ í™”ë©´ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” ! Let's Start ğŸ¤–
"""
)

with st.sidebar:
    video = st.file_uploader("video", type=[".mp4","mov","avi"])

if video:
    chunk_folder = "./.cache/chunks"
    with st.status("Uploading video file...") as status:
        video_content = video.read()
        file_name = video.name
        video_path = f"./.cache/{video.name}"
        format = video_path[-3:]
        audio_path = video_path.replace(format,"mp3")
        transcript_path = video_path.replace(format,"txt")
        with open(video_path,"wb") as f:
            f.write(video_content)
    
        status.update(label="Extracting audio file...")
        extract_audio_from_video(video_path, audio_path)
    
        status.update(label="Cutting audio...")
        cut_audio_in_chunks(audio_path,10,chunk_folder)
        status.update(label="Transcribing audio...")
        transcribe_chunks(chunk_folder, transcript_path)

    transcript, summary, qa = st.tabs(["Transcript","Summary","Q&A"])

    with transcript:
        with open(transcript_path, "r") as file:
            st.write(file.read())
    
    with summary:
        start = st.button("ìš”ì•½ë³¸ ìƒì„±í•˜ê¸° ğŸ“„")

        if start:

            loader = TextLoader(transcript_path)

            splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
                chunk_size = 1000,
                chunk_overlap = 200,
            )

            docs = loader.load_and_split(text_splitter=splitter)
            
            first_prompt = ChatPromptTemplate.from_template(
                """
            Write a concise summary of the following:
            "{text}"
            CONCISE SUMMARY :
            """
            )

            first_chain = first_prompt | llm | StrOutputParser()

            summary = first_chain.invoke({"text": docs[0].page_content})

            refine_prompt = ChatPromptTemplate.from_template(
                """
                Your job is to produce a final summary.
                We have provided an existing summary up to a certain point: {existing_summary}
                We have the opportunity to refine the existing summary (only if needed) with some more context below.
                ------------
                {context}
                ------------
                Given the new context, refine the original summary.
                If the context isn't useful, RETURN the original summary.
                """
            )

            refine_chain = refine_prompt | llm | StrOutputParser()

            with st.status("Summarizing...âœï¸") as status:
                for i, doc in enumerate(docs[1:]):
                    status.update(label=f"Processing document {i+1}/{len(docs)-1}")
                    summary = refine_chain.invoke({"existing_summary":summary,
                                                   "context":doc.page_content},)
                    
                    st.write(summary)
            st.markdown("##### Final Summary ! ğŸ‘‡")
            st.write(summary)


    with qa:

        retriever = embed_file(transcript_path, file_name)
        # doc = retriever.invoke("do they talk about marcus aurelius?")
        # st.write(doc)

        send_message("ì§ˆë¬¸ì„ ê¸°ë‹¤ë¦¬ê³  ìˆìŠµë‹ˆë‹¤ ğŸ¤–", "ai", save=False)
        paint_history()
        message = st.chat_input("ë¹„ë””ì˜¤ íŒŒì¼ ë‚´ìš©ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš” ğŸ™†â€â™€ï¸!")

        if message:
            send_message(message, "human")
            qa_prompt = ChatPromptTemplate.from_messages([
            ("system",
            """
            ì£¼ì–´ì§„ contextë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ë§Œì•½ ë‹µì„ ì˜ ëª¨ë¥´ê² ë‹¤ë©´, 'ì •í™•í•œ ë‹µì„ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤. 
            ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì‹œê² ì–´ìš”?' ë¼ê³  ë‹µë³€í•˜ì„¸ìš”. ì ˆëŒ€ ë‹µë³€ì„ ê¾¸ë©°ë‚´ì§€ ë§ˆì„¸ìš”.

            Context: {context}
            """),
            ("human","{question}"),
            ])

            qa_chain = {"context": retriever|RunnableLambda(format_docs),
                        "question": RunnablePassthrough()} | qa_prompt | llm | StrOutputParser()
            

            response = qa_chain.invoke(message)

            send_message(response, "ai")

else:
    st.session_state["messages"] = []


            
